---
layout:     post
title:      Airflow常用配置及参数
subtitle:   最近写点Airflow的东西
date:       2020-02-11
author:     Rain
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 任务调度
    - Airflow
---


日常工作中，经常需要调度脚本，公司内部使用的是[Airflow](https://airflow.apache.org/docs/stable/)，在此记录一些Airflow中经常用到的重要配置或参数。

## [max_active_runs_per_dag](https://airflow.apache.org/docs/stable/configurations-ref.html#max-active-runs-per-dag)

The maximum number of active DAG runs per DAG：指定Dag可同时运行的最大数量。
> Type
> string

> Default
> 16

> Environment Variable
> AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG


## dagrun_timeout
用于实例化Dag时传参，限定Dag最大执行时间，到了timeout的时间，及时任务还没执行结束，也会直接关闭Dag。

## execution_timeout
与dagrun_timeout相似，限制一个任务最大执行时间。


## start_date, end_date
指定Dag的开始时间和结束时间;
**注意：如果start_date早于当前时间，Airflow会把start_date到当前时间间隔内的任务全部填充执行，当你开始Dag任务执行时，很会就会有max_active_runs_per_dag个Dag同时在执行**,如何防止这种情况发生呢，看下面一个参数

## [catchup_by_default](https://airflow.apache.org/docs/stable/configurations-ref.html#catchup-by-default)
Turn off scheduler catchup by setting this to False. Default behavior is unchanged and Command Line Backfills still work, but the scheduler will not do scheduler catchup if this is False, however it can be set on a per DAG basis in the DAG definition (catchup)
设置为True表示回填当前时间之前的任务，为False则不回填

> Type
> string

> Default
> True

> Environment Variable
> AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT

## dags_folder
The folder where your airflow pipelines live, most likely a subfolder in a code repository. This path must be absolute.
指定Dag文件夹位置（必须是绝对路径），通常是代码库中的子文件夹。

Type
string

Default
{AIRFLOW_HOME}/dags

Environment Variable
AIRFLOW__CORE__DAGS_FOLDER

## default_timezone
Default timezone in case supplied date times are naive can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
默认为TUC时间
> Type
> string

> Default
> utc

> Environment Variable
> AIRFLOW__CORE__DEFAULT_TIMEZONE

## sql_alchemy_conn
The SqlAlchemy connection string to the metadata database. SqlAlchemy supports many different database engine, more information their website

指定SQL_Alchemy连接，用于存储任务数据, Airflow Dag信息。

> Type
> string

> Default
> sqlite:///{AIRFLOW_HOME}/airflow.db

> Environment Variable
> AIRFLOW__CORE__SQL_ALCHEMY_CONN


暂时先记到这里，之后遇到比较重要的参数，再做补充








